import torch
import torch.nn as nn
import torch.optim as optim
from torch.optim.lr_scheduler import LambdaLR
import time
import os
import math
from typing import Dict, List, Tuple
import matplotlib.pyplot as plt

from model import Transformer
from data import DataProcessor
from config import Config
plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

class Trainer:
    """è®­ç»ƒå™¨ç±»"""
    
    def __init__(self, config: Config, model: Transformer, 
                 train_dataloader, val_dataloader=None, vocab_size=None):
        self.config = config
        self.model = model
        self.train_dataloader = train_dataloader
        self.val_dataloader = val_dataloader
        
        # è®¾ç½®è®¾å¤‡
        self.device = torch.device(config.device)
        self.model.to(self.device)
        
        # ä¼˜åŒ–å™¨
        self.optimizer = self._create_optimizer()
        self.scheduler = self._create_scheduler()
        
        # è·å–è¯æ±‡è¡¨å¤§å°
        if vocab_size is None:
            vocab_size = self._get_vocab_size()
        
        # æŸå¤±å‡½æ•°ï¼ˆå¸¦æ ‡ç­¾å¹³æ»‘ï¼‰
        self.criterion = LabelSmoothingLoss(
            vocab_size=vocab_size,
            smoothing=config.training.label_smoothing
        )
        
        # è®­ç»ƒçŠ¶æ€
        self.global_step = 0
        self.epoch = 0
        self.best_val_loss = float('inf')
        
        # è®°å½•è®­ç»ƒå†å²
        self.train_losses = []
        self.val_losses = []
        self.learning_rates = []
        
        print(f"ğŸš€ åˆå§‹åŒ–è®­ç»ƒå™¨ï¼Œè®¾å¤‡: {self.device}, è¯æ±‡è¡¨å¤§å°: {vocab_size}")

    def _get_vocab_size(self):
        """ä»æ¨¡å‹æ¨æ–­è¯æ±‡è¡¨å¤§å°"""
        # æ–¹æ³•1: æ£€æŸ¥è¾“å‡ºæŠ•å½±å±‚
        if hasattr(self.model, 'output_projection'):
            return self.model.output_projection.out_features
        
        # æ–¹æ³•2: æ£€æŸ¥å…¶ä»–å¯èƒ½çš„è¾“å‡ºå±‚åç§°
        possible_names = ['output_layer', 'projection', 'classifier', 'lm_head', 'fc']
        for name in possible_names:
            if hasattr(self.model, name):
                layer = getattr(self.model, name)
                if hasattr(layer, 'out_features'):
                    vocab_size = layer.out_features
                    print(f"âœ… ä» {name} è·å–è¯æ±‡è¡¨å¤§å°: {vocab_size}")
                    return vocab_size
        
        # æ–¹æ³•3: ä»ç›®æ ‡åµŒå…¥å±‚è·å–
        if hasattr(self.model, 'tgt_embedding'):
            vocab_size = self.model.tgt_embedding.num_embeddings
            print(f"ä» tgt_embedding è·å–è¯æ±‡è¡¨å¤§å°: {vocab_size}")
            return vocab_size
        
        # æ–¹æ³•4: ä½¿ç”¨é»˜è®¤å€¼
        print("æ— æ³•æ¨æ–­è¯æ±‡è¡¨å¤§å°ï¼Œä½¿ç”¨é»˜è®¤å€¼ 1000")
        return 1000

    def _create_optimizer(self):
        """åˆ›å»ºä¼˜åŒ–å™¨"""
        return optim.AdamW(
            self.model.parameters(),
            lr=self.config.training.base_learning_rate,
            betas=(self.config.training.adam_beta1, self.config.training.adam_beta2),
            eps=self.config.training.adam_epsilon,
            weight_decay=0.01
        )
    
    def _create_scheduler(self):
        """åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨ï¼ˆå¸¦çº¿æ€§çƒ­èº«ï¼‰"""
        def lr_lambda(step):
            # çº¿æ€§çƒ­èº«ï¼šä»0çº¿æ€§å¢åŠ åˆ°base_learning_rate
            if step < self.config.training.warmup_steps:
                return float(step) / float(max(1, self.config.training.warmup_steps))
            
            # ä½™å¼¦è¡°å‡
            total_steps = self.config.training.max_epochs * len(self.train_dataloader)
            progress = float(step - self.config.training.warmup_steps) / float(
                max(1, total_steps - self.config.training.warmup_steps)
            )
            return max(0.01, 0.5 * (1.0 + math.cos(math.pi * progress)))  # æœ€ä½åˆ°1%çš„åŸºç¡€å­¦ä¹ ç‡
        
        return LambdaLR(self.optimizer, lr_lambda)
    
    def train_epoch(self):
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        total_loss = 0
        total_tokens = 0
        num_batches = 0
        
        start_time = time.time()
        
        for batch_idx, batch in enumerate(self.train_dataloader):
            # ç§»åŠ¨æ•°æ®åˆ°è®¾å¤‡
            src_tokens = batch['src_tokens'].to(self.device)
            tgt_tokens = batch['tgt_tokens'].to(self.device)
            
            # å‰å‘ä¼ æ’­
            self.optimizer.zero_grad()
            output = self.model(src_tokens, tgt_tokens[:, :-1])  # è§£ç å™¨è¾“å…¥å»æ‰æœ€åä¸€ä¸ªtoken
            
            # è®¡ç®—æŸå¤±ï¼ˆç›®æ ‡å»æ‰ç¬¬ä¸€ä¸ªtokenï¼Œå³<bos>ï¼‰
            loss = self.criterion(
                output.contiguous().view(-1, output.size(-1)),
                tgt_tokens[:, 1:].contiguous().view(-1)
            )
            
            # æ£€æŸ¥æŸå¤±æ˜¯å¦åˆç†
            if torch.isnan(loss) or torch.isinf(loss):
                print(f"å¼‚å¸¸æŸå¤±å€¼: {loss.item()}, è·³è¿‡è¯¥æ‰¹æ¬¡")
                continue
            
            # åå‘ä¼ æ’­
            loss.backward()
            
            # æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(
                self.model.parameters(), 
                self.config.training.grad_clip
            )
            
            # ä¼˜åŒ–å™¨æ­¥è¿›
            self.optimizer.step()
            self.scheduler.step()
            
            # ç»Ÿè®¡
            batch_tokens = (tgt_tokens[:, 1:] != 0).sum().item()
            total_loss += loss.item() * batch_tokens
            total_tokens += batch_tokens
            num_batches += 1
            self.global_step += 1
            
            # è®°å½•å­¦ä¹ ç‡
            current_lr = self.scheduler.get_last_lr()[0]
            self.learning_rates.append(current_lr)
            
            # æ¯25%è¿›åº¦æ‰“å°ä¸€æ¬¡
            if batch_idx % max(1, len(self.train_dataloader) // 4) == 0:
                avg_loss = total_loss / total_tokens if total_tokens > 0 else 0
                print(f'Epoch: {self.epoch} [{batch_idx}/{len(self.train_dataloader)}] '
                      f'Loss: {avg_loss:.4f} | LR: {current_lr:.2e} | '
                      f'Tokens: {total_tokens}')
        
        if total_tokens == 0:
            print("æœ¬epochæ²¡æœ‰æœ‰æ•ˆçš„token")
            return float('inf')
            
        epoch_loss = total_loss / total_tokens
        epoch_time = time.time() - start_time
        
        self.train_losses.append(epoch_loss)
        
        print(f'====> Epoch: {self.epoch} è®­ç»ƒå®Œæˆ, å¹³å‡æŸå¤±: {epoch_loss:.4f}, '
              f'æ—¶é—´: {epoch_time:.2f}ç§’, æ€»tokenæ•°: {total_tokens}')
        
        return epoch_loss
    
    def validate(self):
        """éªŒè¯æ¨¡å‹"""
        if self.val_dataloader is None:
            print("æ— éªŒè¯æ•°æ®ï¼Œè·³è¿‡éªŒè¯")
            return None
            
        self.model.eval()
        total_loss = 0
        total_tokens = 0
        num_batches = 0
        
        with torch.no_grad():
            for batch in self.val_dataloader:
                # æ£€æŸ¥æ‰¹æ¬¡æ˜¯å¦ä¸ºç©º
                if batch['src_tokens'].numel() == 0 or batch['tgt_tokens'].numel() == 0:
                    continue
                    
                src_tokens = batch['src_tokens'].to(self.device)
                tgt_tokens = batch['tgt_tokens'].to(self.device)
                
                output = self.model(src_tokens, tgt_tokens[:, :-1])
                loss = self.criterion(
                    output.contiguous().view(-1, output.size(-1)),
                    tgt_tokens[:, 1:].contiguous().view(-1)
                )
                
                batch_tokens = (tgt_tokens[:, 1:] != 0).sum().item()
                total_loss += loss.item() * batch_tokens
                total_tokens += batch_tokens
                num_batches += 1
        
        # å®‰å…¨æ£€æŸ¥
        if num_batches == 0 or total_tokens == 0:
            print("éªŒè¯æ‰¹æ¬¡ä¸ºç©ºï¼Œè·³è¿‡éªŒè¯")
            return None
            
        val_loss = total_loss / total_tokens
        self.val_losses.append(val_loss)
        
        print(f'====> éªŒè¯é›†æŸå¤±: {val_loss:.4f} (åŸºäº {total_tokens} tokens)')
    
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if val_loss < self.best_val_loss:
            self.best_val_loss = val_loss
            self.save_checkpoint(is_best=True)
    
        return val_loss
    
    def train(self):
        """å®Œæ•´è®­ç»ƒå¾ªç¯"""
        print(f"è®­ç»ƒæ•°æ®: {len(self.train_dataloader)} æ‰¹æ¬¡")
        if self.val_dataloader:
            print(f"éªŒè¯æ•°æ®: {len(self.val_dataloader)} æ‰¹æ¬¡")
        
        for epoch in range(self.config.training.max_epochs):
            self.epoch = epoch
            
            # è®­ç»ƒ
            train_loss = self.train_epoch()
            
            # éªŒè¯
            val_loss = self.validate()
            
            # ä¿å­˜æ£€æŸ¥ç‚¹
            if epoch % self.config.training.save_interval == 0:
                self.save_checkpoint()
            
            # ç»˜åˆ¶è®­ç»ƒæ›²çº¿
            if epoch % 5 == 0:  # æ¯5ä¸ªepochç»˜åˆ¶ä¸€æ¬¡
                self.plot_training_progress()
        
        print("è®­ç»ƒå®Œæˆ")
        self.plot_training_progress()
    
    def save_checkpoint(self, is_best=False):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        checkpoint = {
            'epoch': self.epoch,
            'global_step': self.global_step,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'train_losses': self.train_losses,
            'val_losses': self.val_losses,
            'learning_rates': self.learning_rates,
            'best_val_loss': self.best_val_loss,
            'config': self.config.to_dict()
        }
        
        # åˆ›å»ºæ£€æŸ¥ç‚¹ç›®å½•
        os.makedirs('results/checkpoints', exist_ok=True)
        
        # ä¿å­˜å¸¸è§„æ£€æŸ¥ç‚¹
        checkpoint_path = f'results/checkpoints/checkpoint_epoch_{self.epoch}.pt'
        torch.save(checkpoint, checkpoint_path)
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if is_best:
            best_path = 'results/checkpoints/best_model.pt'
            torch.save(checkpoint, best_path)
            print(f" ä¿å­˜æœ€ä½³æ¨¡å‹: {best_path}")
    
    def load_checkpoint(self, checkpoint_path):
        """åŠ è½½æ£€æŸ¥ç‚¹"""
        checkpoint = torch.load(checkpoint_path, map_location=self.device)
        
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
        
        self.epoch = checkpoint['epoch']
        self.global_step = checkpoint['global_step']
        self.train_losses = checkpoint['train_losses']
        self.val_losses = checkpoint['val_losses']
        self.learning_rates = checkpoint['learning_rates']
        self.best_val_loss = checkpoint['best_val_loss']
        
        print(f"åŠ è½½æ£€æŸ¥ç‚¹: epoch {self.epoch}")
    
    def plot_training_progress(self):
        """ç»˜åˆ¶è®­ç»ƒè¿›åº¦å›¾"""
        os.makedirs('results/figures', exist_ok=True)
        
        plt.figure(figsize=(15, 5))
        
        # æŸå¤±æ›²çº¿
        plt.subplot(1, 3, 1)
        epochs = range(1, len(self.train_losses) + 1)
        plt.plot(epochs, self.train_losses, 'b-', label='Training Loss', linewidth=2)
        if self.val_losses:
            plt.plot(epochs, self.val_losses, 'r-', label='Validation Loss', linewidth=2)
        plt.xlabel('Epoch')
        plt.ylabel('Loss')
        plt.title('Training and Validation Loss')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        # å­¦ä¹ ç‡æ›²çº¿
        plt.subplot(1, 3, 2)
        steps = range(len(self.learning_rates))
        # åªæ˜¾ç¤ºå‰1000æ­¥çš„å­¦ä¹ ç‡ï¼Œé¿å…å›¾åƒè¿‡äºå¯†é›†
        display_steps = min(1000, len(self.learning_rates))
        plt.plot(steps[:display_steps], self.learning_rates[:display_steps])
        plt.xlabel('Training Step')
        plt.ylabel('Learning Rate')
        plt.title('Learning Rate Schedule')
        plt.grid(True, alpha=0.3)
        # è®¾ç½®åˆç†çš„yè½´èŒƒå›´
        if self.learning_rates:
            max_lr = max(self.learning_rates[:display_steps])
            plt.ylim(0, max_lr * 1.1)
        
        # æœ€è¿‘å‡ ä¸ªepochçš„è¯¦ç»†æŸå¤±
        plt.subplot(1, 3, 3)
        recent_epochs = min(10, len(self.train_losses))
        if recent_epochs > 0:
            start_idx = max(0, len(self.train_losses) - recent_epochs)
            epochs_range = range(start_idx + 1, len(self.train_losses) + 1)
            plt.plot(epochs_range, self.train_losses[start_idx:], 'b-', label='Training Loss', linewidth=2)
            if self.val_losses and len(self.val_losses) > start_idx:
                plt.plot(epochs_range, self.val_losses[start_idx:], 'r-', label='Validation Loss', linewidth=2)
            plt.xlabel('Epoch')
            plt.ylabel('Loss')
            plt.title(f'Last {recent_epochs} Epochs')
            plt.legend()
            plt.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig('results/figures/training_progress.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        print("Training progress plot saved")

class LabelSmoothingLoss(nn.Module):
    """å¸¦æ ‡ç­¾å¹³æ»‘çš„äº¤å‰ç†µæŸå¤±"""
    
    def __init__(self, vocab_size, smoothing=0.1, ignore_index=0):
        super().__init__()
        self.confidence = 1.0 - smoothing
        self.smoothing = smoothing
        self.vocab_size = vocab_size
        self.ignore_index = ignore_index
        
    def forward(self, output, target):
        # è¾“å‡ºå½¢çŠ¶: (batch_size * seq_len, vocab_size)
        # ç›®æ ‡å½¢çŠ¶: (batch_size * seq_len)
        
        log_probs = torch.nn.functional.log_softmax(output, dim=-1)
        
        # åˆ›å»ºå¹³æ»‘çš„ç›®æ ‡åˆ†å¸ƒ
        true_dist = torch.zeros_like(log_probs)
        true_dist.fill_(self.smoothing / (self.vocab_size - 2))  # -2 æ˜¯ä¸ºäº†æ’é™¤padå’Œå½“å‰token
        true_dist.scatter_(1, target.unsqueeze(1), self.confidence)
        
        # è®¡ç®—æŸå¤±ï¼Œå¿½ç•¥paddingä½ç½®
        mask = (target != self.ignore_index)
        loss = -torch.sum(true_dist * log_probs, dim=-1)
        loss = loss * mask.float()
        
        return loss.sum() / mask.sum()

def calculate_accuracy(output, target, ignore_index=0):
    """è®¡ç®—å‡†ç¡®ç‡ï¼ˆå¿½ç•¥paddingï¼‰"""
    with torch.no_grad():
        # è¾“å‡ºå½¢çŠ¶: (batch_size * seq_len, vocab_size)
        # ç›®æ ‡å½¢çŠ¶: (batch_size * seq_len)
        pred = output.argmax(dim=-1)
        correct = (pred == target)
        
        # å¿½ç•¥paddingä½ç½®
        mask = (target != ignore_index)
        correct = correct & mask
        
        accuracy = correct.float().sum() / mask.float().sum()
        return accuracy.item()

def test_training_small_batch():
    """åœ¨å°æ‰¹é‡æ•°æ®ä¸Šæµ‹è¯•è®­ç»ƒå¾ªç¯"""
    
    try:
        # åŠ è½½é…ç½®
        config = Config.from_yaml('configs/base.yaml')
        
        # åˆ›å»ºæ•°æ®å¤„ç†å™¨
        processor = DataProcessor(
            src_lang=config.data.src_lang,
            tgt_lang=config.data.tgt_lang,
            vocab_size=config.data.vocab_size,
            max_seq_len=config.data.max_seq_len
        )
        
        # åŠ è½½æ•°æ®
        src_sentences, tgt_sentences = processor.load_iwslt2017_dataset()
        print(f"æ•°æ®é›†å¤§å°: {len(src_sentences)} æ¡å¥å­")
        
        # ä½¿ç”¨æ›´å¤šæ•°æ®è®­ç»ƒ
        train_size = min(500, len(src_sentences))
        src_train = src_sentences[:train_size]
        tgt_train = tgt_sentences[:train_size]
        
        # åˆ†å‰²è®­ç»ƒé›†å’ŒéªŒè¯é›†
        split_idx = int(0.8 * train_size)
        src_val = src_train[split_idx:]
        tgt_val = tgt_train[split_idx:]
        src_train = src_train[:split_idx]
        tgt_train = tgt_train[:split_idx]
        
        print(f"è®­ç»ƒé›†: {len(src_train)} æ¡, éªŒè¯é›†: {len(src_val)} æ¡")
        
        # è®­ç»ƒåˆ†è¯å™¨
        processor.train_tokenizers(src_train, tgt_train)
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_dataloader = processor.create_data_loader(
            src_train, tgt_train, 
            batch_size=8, shuffle=True  # å¢å¤§batch_size
        )
        
        val_dataloader = processor.create_data_loader(
            src_val, tgt_val,
            batch_size=8, shuffle=False
        )
        
        print(f"è®­ç»ƒæ‰¹æ¬¡: {len(train_dataloader)}, éªŒè¯æ‰¹æ¬¡: {len(val_dataloader)}")
        
        # åˆ›å»ºæ¨¡å‹
        src_vocab_size = processor.src_tokenizer.get_vocab_size()
        tgt_vocab_size = processor.tgt_tokenizer.get_vocab_size()
        
        print(f"æºè¯­è¨€è¯æ±‡è¡¨å¤§å°: {src_vocab_size}")
        print(f"ç›®æ ‡è¯­è¨€è¯æ±‡è¡¨å¤§å°: {tgt_vocab_size}")
        
        model = Transformer(
            src_vocab_size=src_vocab_size,
            tgt_vocab_size=tgt_vocab_size,
            d_model=config.model.d_model,
            num_heads=config.model.num_heads,
            d_ff=config.model.d_ff,
            num_encoder_layers=config.model.num_encoder_layers,
            num_decoder_layers=config.model.num_decoder_layers,
            max_seq_len=config.model.max_seq_len,
            dropout=config.model.dropout
        )
        
        # è®¡ç®—æ¨¡å‹å‚æ•°
        total_params = sum(p.numel() for p in model.parameters())
        print(f"æ¨¡å‹æ€»å‚æ•°: {total_params:,}")
        
        # åˆ›å»ºè®­ç»ƒå™¨ - æ˜¾å¼ä¼ å…¥è¯æ±‡è¡¨å¤§å°
        trainer = Trainer(config, model, train_dataloader, val_dataloader, vocab_size=tgt_vocab_size)
        
        # æµ‹è¯•å•ä¸ªè®­ç»ƒæ­¥éª¤
        print("\nğŸ”§ æµ‹è¯•å•ä¸ªè®­ç»ƒæ­¥éª¤...")
        test_batch = next(iter(train_dataloader))
        src_tokens = test_batch['src_tokens'].to(trainer.device)
        tgt_tokens = test_batch['tgt_tokens'].to(trainer.device)
        
        # æµ‹è¯•å‰å‘ä¼ æ’­
        model.train()
        output = model(src_tokens, tgt_tokens[:, :-1])
        print(f"æ¨¡å‹è¾“å‡ºå½¢çŠ¶: {output.shape}")
        
        # æµ‹è¯•æŸå¤±è®¡ç®—
        loss = trainer.criterion(
            output.contiguous().view(-1, output.size(-1)),
            tgt_tokens[:, 1:].contiguous().view(-1)
        )
        print(f"åˆå§‹æŸå¤±: {loss.item():.4f}")
        
        # æµ‹è¯•å‡†ç¡®ç‡
        accuracy = calculate_accuracy(
            output.contiguous().view(-1, output.size(-1)),
            tgt_tokens[:, 1:].contiguous().view(-1)
        )
        print(f"åˆå§‹å‡†ç¡®ç‡: {accuracy:.4f}")
        
        # æµ‹è¯•å•ä¸ªè®­ç»ƒæ­¥éª¤
        trainer.optimizer.zero_grad()
        loss.backward()
        trainer.optimizer.step()
        trainer.scheduler.step()
        
        # æµ‹è¯•ä¸€ä¸ªå®Œæ•´çš„epoch
        config.training.max_epochs = 2  # è®­ç»ƒ2ä¸ªepoch
        trainer.train()
        return trainer, processor
        
    except Exception as e:
        print(f"è®­ç»ƒæµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None, None

# ä¿®æ”¹ä¸»å‡½æ•°è°ƒç”¨
if __name__ == "__main__":
    trainer, processor = test_training_small_batch()
    if trainer is None:
        print("è®­ç»ƒæµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯")
        exit(1)